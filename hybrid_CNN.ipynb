{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9b7611e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì XPU (Intel GPU) is available!\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import (Conv2d, ReLU, MaxPool2d, Flatten, Linear, Sequential, \n",
    "                      CrossEntropyLoss)\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from qiskit import QuantumCircuit\n",
    "from qiskit.circuit import Parameter\n",
    "from qiskit_machine_learning.neural_networks import SamplerQNN\n",
    "from qiskit_machine_learning.connectors import TorchConnector\n",
    "import matplotlib.pyplot as plt\n",
    "from qiskit_aer import AerSimulator\n",
    "\n",
    "\n",
    "# Set device to XPU (Intel GPU)print(f\"Device set to: {device}\")\n",
    "\n",
    "if torch.xpu.is_available():\n",
    "\n",
    "    device = torch.device('xpu')\n",
    "\n",
    "    print(\"‚úì XPU (Intel GPU) is available!\")\n",
    "else:\n",
    "    print(\"‚ö† XPU not available, using CPU\")\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5cd00ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select architectures to train on\n",
    "SELECTED_CLASSES = ['dome(inner)', 'dome(outer)', 'gargoyle', 'stained_glass']\n",
    "\n",
    "# Dataset path\n",
    "DATA_DIR = '/home/advik/Quantum/Mini Project/architecture_dataset_32x32'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "618504d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images loaded: 4324\n",
      "Classes: ['dome(inner)', 'dome(outer)', 'gargoyle', 'stained_glass']\n",
      "Images per class: [589, 1175, 1562, 998]\n",
      "\n",
      "Train set: 3459 images\n",
      "Validation set: 865 images\n",
      "\n",
      "Train batches: 433\n",
      "Validation batches: 109\n",
      "‚ö° Batch size reduced to 8 for faster quantum gradient computation\n"
     ]
    }
   ],
   "source": [
    "# Load dataset and create train/validation split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class ArchitectureDataset(Dataset):\n",
    "    def __init__(self, data, labels, transform=None):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "# Define transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Load images and labels\n",
    "train_images = []\n",
    "train_labels = []\n",
    "\n",
    "train_dir = os.path.join(DATA_DIR, 'train')\n",
    "\n",
    "for class_idx, class_name in enumerate(SELECTED_CLASSES):\n",
    "    class_path = os.path.join(train_dir, class_name)\n",
    "    \n",
    "    for img_name in os.listdir(class_path):\n",
    "        img_path = os.path.join(class_path, img_name)\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        train_images.append(img)\n",
    "        train_labels.append(class_idx)\n",
    "\n",
    "print(f\"Total images loaded: {len(train_images)}\")\n",
    "print(f\"Classes: {SELECTED_CLASSES}\")\n",
    "print(f\"Images per class: {[train_labels.count(i) for i in range(len(SELECTED_CLASSES))]}\")\n",
    "\n",
    "# Split train into train and validation (80/20)\n",
    "train_imgs, val_imgs, train_lbls, val_lbls = train_test_split(\n",
    "    train_images, train_labels, test_size=0.2, random_state=42, stratify=train_labels\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain set: {len(train_imgs)} images\")\n",
    "print(f\"Validation set: {len(val_imgs)} images\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = ArchitectureDataset(train_imgs, train_lbls, transform=transform)\n",
    "val_dataset = ArchitectureDataset(val_imgs, val_lbls, transform=transform)\n",
    "\n",
    "# Create dataloaders - REDUCED BATCH SIZE for quantum computing\n",
    "batch_size = 8  # Smaller batches = faster quantum gradient computation\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"\\nTrain batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")\n",
    "print(f\"‚ö° Batch size reduced to {batch_size} for faster quantum gradient computation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6b2a6cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No gradient function provided, creating a gradient function. If your Sampler requires transpilation, please provide a pass manager.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Qiskit Machine Learning quantum layer created!\n",
      "Qubits: 4, Layers: 2, Parameters: 8\n",
      "TorchConnector: ENABLED - Full gradient support\n",
      "Device: xpu\n"
     ]
    }
   ],
   "source": [
    "# Qiskit Machine Learning Quantum Circuit (Proper Implementation)\n",
    "n_qubits = 4\n",
    "n_layers = 2\n",
    "\n",
    "def create_qiskit_qnn():\n",
    "    \"\"\"Create parameterized quantum circuit with Qiskit ML\"\"\"\n",
    "    \n",
    "    # Create parameters\n",
    "    input_params = [Parameter(f'x_{i}') for i in range(n_qubits)]\n",
    "    weight_params = [Parameter(f'Œ∏_{i}') for i in range(n_qubits * n_layers)]\n",
    "    \n",
    "    # Build quantum circuit\n",
    "    qc = QuantumCircuit(n_qubits)\n",
    "    \n",
    "    # Data encoding layer - RY rotations with input features\n",
    "    for i in range(n_qubits):\n",
    "        qc.ry(input_params[i], i)\n",
    "    \n",
    "    # Trainable parameterized layers\n",
    "    param_idx = 0\n",
    "    for layer in range(n_layers):\n",
    "        # Rotation layer\n",
    "        for i in range(n_qubits):\n",
    "            qc.ry(weight_params[param_idx], i)\n",
    "            param_idx += 1\n",
    "        \n",
    "        # Entanglement layer (circular)\n",
    "        for i in range(n_qubits - 1):\n",
    "            qc.cx(i, i + 1)\n",
    "        qc.cx(n_qubits - 1, 0)\n",
    "    \n",
    "    # Create SamplerQNN with gradient support\n",
    "    qnn = SamplerQNN(\n",
    "        circuit=qc,\n",
    "        input_params=input_params,\n",
    "        weight_params=weight_params,\n",
    "        input_gradients=True  # Enable gradient computation\n",
    "    )\n",
    "    \n",
    "    # Wrap with TorchConnector for PyTorch integration\n",
    "    quantum_layer = TorchConnector(qnn)\n",
    "    \n",
    "    return quantum_layer\n",
    "\n",
    "# Create the quantum layer\n",
    "quantum_layer = create_qiskit_qnn()\n",
    "\n",
    "print(\"‚úì Qiskit Machine Learning quantum layer created!\")\n",
    "print(f\"Qubits: {n_qubits}, Layers: {n_layers}, Parameters: {n_qubits * n_layers}\")\n",
    "print(f\"TorchConnector: ENABLED - Full gradient support\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "88a5287f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HybridQNN(\n",
      "  (feature_extractor): Sequential(\n",
      "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Flatten(start_dim=1, end_dim=-1)\n",
      "    (7): Linear(in_features=2048, out_features=64, bias=True)\n",
      "    (8): ReLU()\n",
      "    (9): Linear(in_features=64, out_features=4, bias=True)\n",
      "  )\n",
      "  (quantum_layer): TorchConnector()\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.2, inplace=False)\n",
      "    (3): Linear(in_features=32, out_features=4, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Total parameters: 137168\n",
      "Model device: xpu:0\n",
      "\n",
      "‚úì All components are differentiable - gradients will flow!\n"
     ]
    }
   ],
   "source": [
    "# Hybrid Quantum-Classical Model with Qiskit ML\n",
    "class HybridQNN(nn.Module):\n",
    "    def __init__(self, quantum_layer, n_qubits=4, n_classes=4):\n",
    "        super(HybridQNN, self).__init__()\n",
    "        \n",
    "        # Simple CNN feature extractor (32x32x3 -> n_qubits)\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 16x16\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 8x8\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32 * 8 * 8, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, n_qubits)  # Map to n_qubits inputs\n",
    "        )\n",
    "        \n",
    "        # Quantum layer (TorchConnector handles gradients automatically!)\n",
    "        self.quantum_layer = quantum_layer\n",
    "        \n",
    "        # Classical classifier (SamplerQNN outputs 2^n_qubits probabilities)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(2**n_qubits, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(32, n_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Extract classical features\n",
    "        features = self.feature_extractor(x)  # (batch, n_qubits)\n",
    "        features = torch.tanh(features) * np.pi  # Scale to quantum rotation range\n",
    "        \n",
    "        # Process through quantum layer (fully differentiable!)\n",
    "        quantum_outputs = self.quantum_layer(features)\n",
    "        \n",
    "        # Final classification\n",
    "        output = self.classifier(quantum_outputs)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Create model with Qiskit ML quantum layer\n",
    "model = HybridQNN(quantum_layer=quantum_layer, n_qubits=n_qubits, n_classes=len(SELECTED_CLASSES))\n",
    "model.to(device)\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "print(f\"Model device: {next(model.parameters()).device}\")\n",
    "print(\"\\n‚úì All components are differentiable - gradients will flow!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f253d160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training functions ready!\n"
     ]
    }
   ],
   "source": [
    "# Training setup\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        if batch_idx % 5 == 0:\n",
    "            print(f'Batch {batch_idx}/{len(loader)}, Loss: {loss.item():.4f}')\n",
    "    \n",
    "    accuracy = 100. * correct / total\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def validate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    accuracy = 100. * correct / total\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "print(\"Training functions ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6dca60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for 10 epochs...\n",
      "\n",
      "‚ö†Ô∏è Note: Quantum gradient computation is slow (parameter shift rule)\n",
      "   Each batch processes quantum circuits multiple times for gradients\n",
      "\n",
      "Epoch 1/10\n",
      "--------------------------------------------------\n",
      "Batch 0/433, Loss: 1.3701\n",
      "Batch 5/433, Loss: 1.3872\n",
      "Batch 10/433, Loss: 1.3818\n",
      "Batch 15/433, Loss: 1.4029\n",
      "Batch 20/433, Loss: 1.3390\n",
      "Batch 25/433, Loss: 1.3124\n",
      "Batch 30/433, Loss: 1.2938\n",
      "Batch 35/433, Loss: 1.3936\n",
      "Batch 40/433, Loss: 1.3575\n",
      "Batch 45/433, Loss: 1.3574\n",
      "Batch 50/433, Loss: 1.4073\n",
      "Batch 55/433, Loss: 1.3483\n",
      "Batch 60/433, Loss: 1.3086\n",
      "Batch 65/433, Loss: 1.3903\n",
      "Batch 70/433, Loss: 1.2880\n",
      "Batch 75/433, Loss: 1.3881\n",
      "Batch 80/433, Loss: 1.2352\n",
      "Batch 85/433, Loss: 1.2849\n",
      "Batch 90/433, Loss: 1.2003\n",
      "Batch 95/433, Loss: 1.2094\n",
      "Batch 100/433, Loss: 1.2703\n",
      "Batch 105/433, Loss: 1.2248\n",
      "Batch 110/433, Loss: 1.4097\n",
      "Batch 115/433, Loss: 1.3290\n",
      "Batch 120/433, Loss: 1.2333\n",
      "Batch 125/433, Loss: 1.1338\n",
      "Batch 130/433, Loss: 1.1821\n",
      "Batch 135/433, Loss: 1.1812\n",
      "Batch 140/433, Loss: 1.1306\n",
      "Batch 145/433, Loss: 1.2342\n",
      "Batch 150/433, Loss: 1.1925\n",
      "Batch 155/433, Loss: 1.3024\n",
      "Batch 160/433, Loss: 1.1410\n",
      "Batch 165/433, Loss: 1.1090\n",
      "Batch 170/433, Loss: 1.2296\n",
      "Batch 175/433, Loss: 1.1207\n",
      "Batch 180/433, Loss: 1.2535\n",
      "Batch 185/433, Loss: 1.2729\n",
      "Batch 190/433, Loss: 1.0399\n",
      "Batch 195/433, Loss: 1.0221\n",
      "Batch 200/433, Loss: 1.4246\n",
      "Batch 205/433, Loss: 1.1257\n",
      "Batch 210/433, Loss: 1.0578\n",
      "Batch 215/433, Loss: 1.1358\n",
      "Batch 220/433, Loss: 0.9919\n",
      "Batch 225/433, Loss: 1.0112\n",
      "Batch 230/433, Loss: 0.9842\n",
      "Batch 235/433, Loss: 1.0708\n",
      "Batch 240/433, Loss: 1.1121\n",
      "Batch 245/433, Loss: 1.0619\n",
      "Batch 250/433, Loss: 1.2180\n",
      "Batch 255/433, Loss: 0.9309\n",
      "Batch 260/433, Loss: 1.3053\n",
      "Batch 265/433, Loss: 1.0203\n",
      "Batch 270/433, Loss: 1.0711\n",
      "Batch 275/433, Loss: 1.0860\n",
      "Batch 280/433, Loss: 1.0339\n",
      "Batch 285/433, Loss: 1.1441\n",
      "Batch 290/433, Loss: 0.8530\n",
      "Batch 295/433, Loss: 1.1263\n",
      "Batch 300/433, Loss: 1.1708\n",
      "Batch 305/433, Loss: 1.1554\n",
      "Batch 310/433, Loss: 1.1258\n",
      "Batch 315/433, Loss: 1.2238\n",
      "Batch 320/433, Loss: 0.9975\n",
      "Batch 325/433, Loss: 0.9824\n",
      "Batch 330/433, Loss: 1.0246\n",
      "Batch 335/433, Loss: 0.9888\n",
      "Batch 340/433, Loss: 1.0670\n",
      "Batch 345/433, Loss: 1.1283\n",
      "Batch 350/433, Loss: 1.0284\n",
      "Batch 355/433, Loss: 0.8772\n",
      "Batch 360/433, Loss: 0.8779\n",
      "Batch 365/433, Loss: 0.8241\n",
      "Batch 370/433, Loss: 0.8652\n",
      "Batch 375/433, Loss: 1.2127\n",
      "Batch 380/433, Loss: 0.6750\n",
      "Batch 385/433, Loss: 1.0484\n",
      "Batch 390/433, Loss: 1.0809\n",
      "Batch 395/433, Loss: 0.9484\n",
      "Batch 400/433, Loss: 0.8897\n",
      "Batch 405/433, Loss: 0.6541\n",
      "Batch 410/433, Loss: 0.7422\n",
      "Batch 415/433, Loss: 0.9755\n",
      "Batch 420/433, Loss: 0.5003\n",
      "Batch 425/433, Loss: 0.9131\n",
      "Batch 430/433, Loss: 0.8278\n",
      "\n",
      "Epoch 1 Summary:\n",
      "Train Loss: 1.1178, Train Acc: 50.97%\n",
      "Val Loss: 0.8709, Val Acc: 62.08%\n",
      "==================================================\n",
      "\n",
      "Epoch 2/10\n",
      "--------------------------------------------------\n",
      "Batch 0/433, Loss: 1.1651\n",
      "Batch 5/433, Loss: 1.2131\n",
      "Batch 10/433, Loss: 0.9858\n",
      "Batch 15/433, Loss: 0.6679\n",
      "Batch 20/433, Loss: 0.7275\n",
      "Batch 25/433, Loss: 1.1195\n",
      "Batch 30/433, Loss: 0.6284\n",
      "Batch 35/433, Loss: 0.6950\n",
      "Batch 40/433, Loss: 1.0047\n",
      "Batch 45/433, Loss: 0.8509\n",
      "Batch 50/433, Loss: 0.9028\n",
      "Batch 55/433, Loss: 0.6405\n",
      "Batch 60/433, Loss: 1.1971\n",
      "Batch 65/433, Loss: 1.0929\n",
      "Batch 70/433, Loss: 0.8021\n",
      "Batch 75/433, Loss: 0.8442\n",
      "Batch 80/433, Loss: 0.6853\n",
      "Batch 85/433, Loss: 0.6975\n",
      "Batch 90/433, Loss: 0.7327\n",
      "Batch 95/433, Loss: 0.7588\n",
      "Batch 100/433, Loss: 0.7144\n",
      "Batch 105/433, Loss: 0.8480\n",
      "Batch 110/433, Loss: 0.7148\n",
      "Batch 115/433, Loss: 0.5048\n",
      "Batch 120/433, Loss: 0.5788\n",
      "Batch 125/433, Loss: 0.7884\n",
      "Batch 130/433, Loss: 0.6829\n",
      "Batch 135/433, Loss: 0.7953\n",
      "Batch 140/433, Loss: 0.7707\n",
      "Batch 145/433, Loss: 0.5028\n",
      "Batch 150/433, Loss: 0.4610\n",
      "Batch 155/433, Loss: 0.8826\n",
      "Batch 160/433, Loss: 0.6940\n",
      "Batch 165/433, Loss: 1.1388\n",
      "Batch 170/433, Loss: 0.9977\n",
      "Batch 175/433, Loss: 0.8347\n",
      "Batch 180/433, Loss: 0.6875\n",
      "Batch 185/433, Loss: 0.7222\n",
      "Batch 190/433, Loss: 0.6611\n",
      "Batch 195/433, Loss: 0.7753\n",
      "Batch 200/433, Loss: 0.8777\n",
      "Batch 205/433, Loss: 0.6832\n",
      "Batch 210/433, Loss: 0.6264\n",
      "Batch 215/433, Loss: 0.4780\n",
      "Batch 220/433, Loss: 0.8583\n",
      "Batch 225/433, Loss: 0.6195\n",
      "Batch 230/433, Loss: 0.7599\n",
      "Batch 235/433, Loss: 1.0818\n",
      "Batch 240/433, Loss: 0.2571\n",
      "Batch 245/433, Loss: 0.9184\n",
      "Batch 250/433, Loss: 0.6454\n",
      "Batch 255/433, Loss: 0.7384\n",
      "Batch 260/433, Loss: 0.5153\n",
      "Batch 265/433, Loss: 0.5796\n",
      "Batch 270/433, Loss: 0.5614\n",
      "Batch 275/433, Loss: 0.9609\n",
      "Batch 280/433, Loss: 0.7999\n",
      "Batch 285/433, Loss: 0.9330\n",
      "Batch 290/433, Loss: 1.3034\n",
      "Batch 295/433, Loss: 0.8269\n",
      "Batch 300/433, Loss: 0.6416\n",
      "Batch 305/433, Loss: 0.6579\n",
      "Batch 310/433, Loss: 0.3535\n",
      "Batch 315/433, Loss: 0.7678\n",
      "Batch 320/433, Loss: 1.1522\n",
      "Batch 325/433, Loss: 0.9725\n",
      "Batch 330/433, Loss: 0.6219\n",
      "Batch 335/433, Loss: 0.8553\n",
      "Batch 340/433, Loss: 0.6597\n",
      "Batch 345/433, Loss: 0.4495\n",
      "Batch 350/433, Loss: 0.5456\n",
      "Batch 355/433, Loss: 0.9318\n",
      "Batch 360/433, Loss: 0.7097\n",
      "Batch 365/433, Loss: 0.5152\n",
      "Batch 370/433, Loss: 0.5713\n",
      "Batch 375/433, Loss: 0.5979\n",
      "Batch 380/433, Loss: 0.7306\n",
      "Batch 385/433, Loss: 1.0796\n",
      "Batch 390/433, Loss: 0.5215\n",
      "Batch 395/433, Loss: 0.7428\n",
      "Batch 400/433, Loss: 0.5638\n",
      "Batch 405/433, Loss: 0.3865\n",
      "Batch 410/433, Loss: 1.0570\n",
      "Batch 415/433, Loss: 0.6966\n",
      "Batch 420/433, Loss: 0.5032\n",
      "Batch 425/433, Loss: 0.7214\n",
      "Batch 430/433, Loss: 0.6118\n",
      "\n",
      "Epoch 2 Summary:\n",
      "Train Loss: 0.7467, Train Acc: 63.63%\n",
      "Val Loss: 0.6886, Val Acc: 64.39%\n",
      "==================================================\n",
      "\n",
      "Epoch 3/10\n",
      "--------------------------------------------------\n",
      "Batch 0/433, Loss: 0.6389\n",
      "Batch 5/433, Loss: 0.5857\n",
      "Batch 10/433, Loss: 0.5438\n",
      "Batch 15/433, Loss: 0.4585\n",
      "Batch 20/433, Loss: 0.6135\n",
      "Batch 25/433, Loss: 0.6860\n",
      "Batch 30/433, Loss: 0.6765\n",
      "Batch 35/433, Loss: 0.7099\n",
      "Batch 40/433, Loss: 0.4418\n",
      "Batch 45/433, Loss: 0.5683\n",
      "Batch 50/433, Loss: 0.4732\n",
      "Batch 55/433, Loss: 0.6429\n",
      "Batch 60/433, Loss: 0.8816\n",
      "Batch 65/433, Loss: 0.4974\n",
      "Batch 70/433, Loss: 0.8549\n",
      "Batch 75/433, Loss: 0.9417\n",
      "Batch 80/433, Loss: 0.6086\n",
      "Batch 85/433, Loss: 0.7035\n",
      "Batch 90/433, Loss: 0.6390\n",
      "Batch 95/433, Loss: 0.6401\n",
      "Batch 100/433, Loss: 0.6338\n",
      "Batch 105/433, Loss: 0.5872\n",
      "Batch 110/433, Loss: 0.6844\n",
      "Batch 115/433, Loss: 0.7445\n",
      "Batch 120/433, Loss: 0.8617\n",
      "Batch 125/433, Loss: 0.5764\n",
      "Batch 130/433, Loss: 0.5563\n",
      "Batch 135/433, Loss: 0.4475\n",
      "Batch 140/433, Loss: 0.8408\n",
      "Batch 145/433, Loss: 0.5344\n",
      "Batch 150/433, Loss: 0.4896\n",
      "Batch 155/433, Loss: 0.4789\n",
      "Batch 160/433, Loss: 0.5354\n",
      "Batch 165/433, Loss: 0.7559\n",
      "Batch 170/433, Loss: 0.6637\n",
      "Batch 175/433, Loss: 0.7420\n",
      "Batch 180/433, Loss: 0.6644\n",
      "Batch 185/433, Loss: 0.2968\n",
      "Batch 190/433, Loss: 0.6234\n",
      "Batch 195/433, Loss: 0.4702\n",
      "Batch 200/433, Loss: 0.4638\n",
      "Batch 205/433, Loss: 0.6671\n",
      "Batch 210/433, Loss: 1.0830\n",
      "Batch 215/433, Loss: 0.6032\n",
      "Batch 220/433, Loss: 0.4499\n",
      "Batch 225/433, Loss: 0.5148\n",
      "Batch 230/433, Loss: 0.5850\n",
      "Batch 235/433, Loss: 0.6186\n",
      "Batch 240/433, Loss: 0.9660\n",
      "Batch 245/433, Loss: 0.6054\n",
      "Batch 250/433, Loss: 0.9339\n",
      "Batch 255/433, Loss: 0.5508\n",
      "Batch 260/433, Loss: 0.7708\n",
      "Batch 265/433, Loss: 1.0877\n",
      "Batch 270/433, Loss: 1.2172\n",
      "Batch 275/433, Loss: 0.6487\n",
      "Batch 280/433, Loss: 0.5750\n",
      "Batch 285/433, Loss: 0.4375\n",
      "Batch 290/433, Loss: 0.5952\n",
      "Batch 295/433, Loss: 0.9226\n",
      "Batch 300/433, Loss: 0.6522\n",
      "Batch 305/433, Loss: 0.5561\n",
      "Batch 310/433, Loss: 0.9113\n",
      "Batch 315/433, Loss: 0.8133\n",
      "Batch 365/433, Loss: 0.7869\n",
      "Batch 370/433, Loss: 0.4965\n",
      "Batch 375/433, Loss: 0.2826\n",
      "Batch 380/433, Loss: 1.9393\n",
      "Batch 385/433, Loss: 0.3007\n",
      "Batch 390/433, Loss: 0.8743\n",
      "Batch 395/433, Loss: 0.9824\n",
      "Batch 400/433, Loss: 0.6594\n",
      "Batch 405/433, Loss: 0.2648\n",
      "Batch 410/433, Loss: 1.1120\n",
      "Batch 415/433, Loss: 0.5372\n",
      "Batch 420/433, Loss: 0.5346\n",
      "Batch 425/433, Loss: 0.8139\n",
      "Batch 430/433, Loss: 0.7989\n",
      "\n",
      "Epoch 3 Summary:\n",
      "Train Loss: 0.6536, Train Acc: 69.73%\n",
      "Val Loss: 0.6850, Val Acc: 74.34%\n",
      "==================================================\n",
      "\n",
      "Epoch 4/10\n",
      "--------------------------------------------------\n",
      "Batch 0/433, Loss: 0.7423\n",
      "Batch 5/433, Loss: 1.7638\n",
      "Batch 10/433, Loss: 0.6392\n",
      "Batch 15/433, Loss: 0.6053\n",
      "Batch 20/433, Loss: 0.2865\n",
      "Batch 25/433, Loss: 0.5450\n",
      "Batch 30/433, Loss: 0.5384\n",
      "Batch 35/433, Loss: 0.3877\n",
      "Batch 40/433, Loss: 0.4594\n",
      "Batch 45/433, Loss: 0.4895\n",
      "Batch 50/433, Loss: 0.8747\n",
      "Batch 55/433, Loss: 0.7624\n",
      "Batch 60/433, Loss: 0.5233\n",
      "Batch 65/433, Loss: 0.5240\n",
      "Batch 70/433, Loss: 0.5114\n",
      "Batch 75/433, Loss: 0.4771\n",
      "Batch 80/433, Loss: 0.6208\n",
      "Batch 85/433, Loss: 0.3083\n",
      "Batch 90/433, Loss: 0.5828\n",
      "Batch 95/433, Loss: 0.5407\n",
      "Batch 100/433, Loss: 1.0903\n",
      "Batch 105/433, Loss: 0.8295\n",
      "Batch 110/433, Loss: 0.5632\n",
      "Batch 115/433, Loss: 0.3827\n",
      "Batch 120/433, Loss: 0.4705\n",
      "Batch 125/433, Loss: 0.5444\n",
      "Batch 130/433, Loss: 0.5017\n",
      "Batch 135/433, Loss: 0.6037\n",
      "Batch 140/433, Loss: 0.5511\n",
      "Batch 145/433, Loss: 0.7040\n",
      "Batch 150/433, Loss: 0.5612\n",
      "Batch 155/433, Loss: 1.2543\n",
      "Batch 160/433, Loss: 0.7192\n",
      "Batch 165/433, Loss: 0.4542\n",
      "Batch 170/433, Loss: 0.4073\n",
      "Batch 175/433, Loss: 0.1945\n",
      "Batch 180/433, Loss: 0.5515\n",
      "Batch 185/433, Loss: 0.4796\n",
      "Batch 190/433, Loss: 0.3465\n",
      "Batch 195/433, Loss: 0.4318\n"
     ]
    }
   ],
   "source": [
    "# Train the model with live plotting\n",
    "n_epochs = 5  # Reduced for quantum - each epoch takes longer\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accs = []\n",
    "val_accs = []\n",
    "\n",
    "print(f\"Starting training for {n_epochs} epochs...\\n\")\n",
    "print(\"‚ö†Ô∏è Note: Quantum gradient computation is slow (parameter shift rule)\")\n",
    "print(\"   Each batch processes quantum circuits multiple times for gradients\\n\")\n",
    "\n",
    "# Setup live plotting\n",
    "%matplotlib inline\n",
    "from IPython import display\n",
    "\n",
    "# Create figure\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc = validate(model, val_loader, criterion)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch+1} Summary:\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "    print(\"=\" * 50 + \"\\n\")\n",
    "    \n",
    "    # Update plots in real-time\n",
    "    ax1.clear()\n",
    "    ax2.clear()\n",
    "    \n",
    "    # Plot loss\n",
    "    ax1.plot(range(1, epoch+2), train_losses, 'b-o', label='Train Loss', linewidth=2, markersize=6)\n",
    "    ax1.plot(range(1, epoch+2), val_losses, 'r-s', label='Val Loss', linewidth=2, markersize=6)\n",
    "    ax1.set_xlabel('Epoch', fontsize=11)\n",
    "    ax1.set_ylabel('Loss', fontsize=11)\n",
    "    ax1.set_title('Training and Validation Loss', fontsize=12, fontweight='bold')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot accuracy\n",
    "    ax2.plot(range(1, epoch+2), train_accs, 'b-o', label='Train Acc', linewidth=2, markersize=6)\n",
    "    ax2.plot(range(1, epoch+2), val_accs, 'r-s', label='Val Acc', linewidth=2, markersize=6)\n",
    "    ax2.set_xlabel('Epoch', fontsize=11)\n",
    "    ax2.set_ylabel('Accuracy (%)', fontsize=11)\n",
    "    ax2.set_title('Training and Validation Accuracy', fontsize=12, fontweight='bold')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Update display\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(fig)\n",
    "\n",
    "print(f\"\\nüéØ Final Results:\")\n",
    "print(f\"Best Val Accuracy: {max(val_accs):.2f}%\")\n",
    "print(f\"Final Train Accuracy: {train_accs[-1]:.2f}%\")\n",
    "print(f\"Final Val Accuracy: {val_accs[-1]:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".qcompute (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
