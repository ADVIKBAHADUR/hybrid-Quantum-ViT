{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b7611e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XPU (Intel GPU) is available!\n",
      "Device set to: xpu\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "from qiskit import QuantumCircuit\n",
    "from qiskit.circuit import Parameter\n",
    "from qiskit_machine_learning.neural_networks import SamplerQNN\n",
    "from qiskit_machine_learning.connectors import TorchConnector\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Set device\n",
    "if torch.xpu.is_available():\n",
    "    device = torch.device('xpu')\n",
    "    print(\"XPU (Intel GPU) is available!\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"XPU not available, using CPU\")\n",
    "\n",
    "print(f\"Device set to: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5cd00ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select architectures to train on\n",
    "SELECTED_CLASSES = ['dome(inner)', 'dome(outer)', 'gargoyle', 'stained_glass']\n",
    "\n",
    "# Dataset path\n",
    "DATA_DIR = '/home/advik/Quantum/Mini Project/architecture_dataset_32x32'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "618504d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images loaded: 4324\n",
      "Classes: ['dome(inner)', 'dome(outer)', 'gargoyle', 'stained_glass']\n",
      "Images per class: [589, 1175, 1562, 998]\n",
      "\n",
      "Train set: 3459 images\n",
      "Validation set: 865 images\n",
      "\n",
      "Train batches: 433\n",
      "Validation batches: 109\n",
      "âš¡ Batch size reduced to 8 for faster quantum gradient computation\n"
     ]
    }
   ],
   "source": [
    "# Load dataset and create train/validation split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class ArchitectureDataset(Dataset):\n",
    "    def __init__(self, data, labels, transform=None):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "# Define transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Load images and labels\n",
    "train_images = []\n",
    "train_labels = []\n",
    "\n",
    "train_dir = os.path.join(DATA_DIR, 'train')\n",
    "\n",
    "for class_idx, class_name in enumerate(SELECTED_CLASSES):\n",
    "    class_path = os.path.join(train_dir, class_name)\n",
    "    \n",
    "    for img_name in os.listdir(class_path):\n",
    "        img_path = os.path.join(class_path, img_name)\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        train_images.append(img)\n",
    "        train_labels.append(class_idx)\n",
    "\n",
    "print(f\"Total images loaded: {len(train_images)}\")\n",
    "print(f\"Classes: {SELECTED_CLASSES}\")\n",
    "print(f\"Images per class: {[train_labels.count(i) for i in range(len(SELECTED_CLASSES))]}\")\n",
    "\n",
    "# Split train into train and validation (80/20)\n",
    "train_imgs, val_imgs, train_lbls, val_lbls = train_test_split(\n",
    "    train_images, train_labels, test_size=0.2, random_state=42, stratify=train_labels\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain set: {len(train_imgs)} images\")\n",
    "print(f\"Validation set: {len(val_imgs)} images\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = ArchitectureDataset(train_imgs, train_lbls, transform=transform)\n",
    "val_dataset = ArchitectureDataset(val_imgs, val_lbls, transform=transform)\n",
    "\n",
    "# Create dataloaders - REDUCED BATCH SIZE for quantum computing\n",
    "batch_size = 8  # Smaller batches = faster quantum gradient computation\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"\\nTrain batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")\n",
    "print(f\"âš¡ Batch size reduced to {batch_size} for faster quantum gradient computation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b2a6cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No gradient function provided, creating a gradient function. If your Sampler requires transpilation, please provide a pass manager.\n",
      "No gradient function provided, creating a gradient function. If your Sampler requires transpilation, please provide a pass manager.\n",
      "No gradient function provided, creating a gradient function. If your Sampler requires transpilation, please provide a pass manager.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Quantum QKV Projection Circuits...\n",
      "============================================================\n",
      "Qubits per circuit:       4\n",
      "Variational layers:       2\n",
      "Trainable params/circuit: 24\n",
      "Output dim per circuit:   2^4 = 16\n",
      "Total quantum circuits:   3 (Q, K, V)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Quantum Circuits for QKV Projections\n",
    "# ==============================================================================\n",
    "# These parameterized quantum circuits replace the classical Linear layers\n",
    "# that normally compute Q, K, V in transformer self-attention.\n",
    "#\n",
    "# Design choices (aligned with proposal):\n",
    "#   - 4 qubits  -> 2^4 = 16 output dimensions (probability distribution)\n",
    "#   - 2 variational layers -> 24 trainable parameters per circuit\n",
    "#   - Angle encoding (RY) for classical-to-quantum data input\n",
    "#   - Circular CNOT entanglement for inter-qubit correlations\n",
    "#   - SamplerQNN + TorchConnector for full PyTorch gradient integration\n",
    "# ==============================================================================\n",
    "\n",
    "n_qubits = 4   # 4 qubits -> output dim 2^4 = 16\n",
    "n_layers = 2   # Shallow circuit: avoids barren plateaus, faster gradients\n",
    "\n",
    "def create_qkv_quantum_circuit(name='Q'):\n",
    "    \"\"\"\n",
    "    Build a parameterized quantum circuit for one of Q/K/V projections.\n",
    "    \n",
    "    Input:  n_qubits real values (angle-encoded via RY gates)\n",
    "    Output: 2^n_qubits probability distribution (measured in computational basis)\n",
    "    \"\"\"\n",
    "    # Input parameters (data encoding)\n",
    "    input_params = [Parameter(f'{name}_x_{i}') for i in range(n_qubits)]\n",
    "    # Trainable weight parameters: 3 rotations (RY, RZ, RX) per qubit per layer\n",
    "    weight_params = [Parameter(f'{name}_w_{i}') for i in range(n_qubits * n_layers * 3)]\n",
    "    \n",
    "    qc = QuantumCircuit(n_qubits)\n",
    "    \n",
    "    # === Data encoding layer ===\n",
    "    for i in range(n_qubits):\n",
    "        qc.ry(input_params[i], i)\n",
    "    \n",
    "    # === Variational layers ===\n",
    "    param_idx = 0\n",
    "    for layer in range(n_layers):\n",
    "        # RY rotation\n",
    "        for i in range(n_qubits):\n",
    "            qc.ry(weight_params[param_idx], i)\n",
    "            param_idx += 1\n",
    "        # RZ rotation\n",
    "        for i in range(n_qubits):\n",
    "            qc.rz(weight_params[param_idx], i)\n",
    "            param_idx += 1\n",
    "        # Circular CNOT entanglement\n",
    "        for i in range(n_qubits - 1):\n",
    "            qc.cx(i, i + 1)\n",
    "        qc.cx(n_qubits - 1, 0)\n",
    "        # RX rotation after entanglement\n",
    "        for i in range(n_qubits):\n",
    "            qc.rx(weight_params[param_idx], i)\n",
    "            param_idx += 1\n",
    "    \n",
    "    # Wrap as a PyTorch-compatible quantum layer\n",
    "    qnn = SamplerQNN(\n",
    "        circuit=qc,\n",
    "        input_params=input_params,\n",
    "        weight_params=weight_params,\n",
    "        input_gradients=True  # REQUIRED for hybrid gradient backprop\n",
    "    )\n",
    "    return TorchConnector(qnn)\n",
    "\n",
    "# Create 3 separate quantum circuits: one each for Q, K, V\n",
    "print(\"Creating Quantum QKV Projection Circuits...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "quantum_q = create_qkv_quantum_circuit('Q')\n",
    "quantum_k = create_qkv_quantum_circuit('K')\n",
    "quantum_v = create_qkv_quantum_circuit('V')\n",
    "\n",
    "print(f\"Qubits per circuit:       {n_qubits}\")\n",
    "print(f\"Variational layers:       {n_layers}\")\n",
    "print(f\"Trainable params/circuit: {n_qubits * n_layers * 3}\")\n",
    "print(f\"Output dim per circuit:   2^{n_qubits} = {2**n_qubits}\")\n",
    "print(f\"Total quantum circuits:   3 (Q, K, V)\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88a5287f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "HYBRID QUANTUM-CLASSICAL VISION TRANSFORMER\n",
      "============================================================\n",
      "Patch size:            16x16 -> 4 patches per image\n",
      "Embedding dim:         64\n",
      "Attention heads:       4 (head_dim=16)\n",
      "Block 1 (Hybrid):      Quantum QKV (4q, 16 output) + Classical attention\n",
      "Block 2 (Classical):   Standard self-attention\n",
      "Classification:        MLP (64 -> 128 -> 4)\n",
      "------------------------------------------------------------\n",
      "Total parameters:      117,016\n",
      "  Quantum parameters:  72 (Q + K + V circuits)\n",
      "  Classical parameters:116,944\n",
      "Quantum evals/image:   4 patches x 3 circuits = 12 âš¡\n",
      "Device:                xpu\n",
      "ðŸš€ 4Ã— SPEEDUP from larger patches (4 vs 16 patches)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Hybrid Quantum-Classical Vision Transformer\n",
    "# ==============================================================================\n",
    "# Architecture (matching proposal Figure 1):\n",
    "#\n",
    "#   [Input 32x32 RGB] \n",
    "#       -> Patch Embedding (Classical: Conv2d)\n",
    "#       -> Positional Encoding (Classical: learnable)\n",
    "#       -> Transformer Block 1 (HYBRID):\n",
    "#           - Self-Attention with Quantum QKV projections\n",
    "#             * pre-projection:  Linear embed_dim -> n_qubits  (Classical)\n",
    "#             * Q, K, V circuits: SamplerQNN via TorchConnector (QUANTUM)\n",
    "#             * post-projection: Linear quantum_out -> embed_dim (Classical)\n",
    "#             * attention scores: softmax(QK^T / sqrt(d))       (Classical)\n",
    "#             * weighted values:  attn @ V                      (Classical)\n",
    "#           - Feed-Forward Network                              (Classical)\n",
    "#           - LayerNorm + Residual connections                  (Classical)\n",
    "#       -> Transformer Block 2 (CLASSICAL):\n",
    "#           - Standard self-attention (Linear QKV)\n",
    "#           - Feed-Forward Network\n",
    "#           - LayerNorm + Residual connections\n",
    "#       -> Classification Head (Classical: MLP)\n",
    "#\n",
    "# Key performance choices:\n",
    "#   - patch_size=16 -> 4 patches (not 64), reducing quantum circuit calls by 4Ã—\n",
    "#   - Only Block 1 uses quantum QKV; Block 2 is fully classical\n",
    "#   - 4 qubits -> 16 output dims per circuit (not 256)\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"Split image into patches and project to embedding dim (Classical)\"\"\"\n",
    "    def __init__(self, img_size=32, patch_size=8, in_channels=3, embed_dim=64):\n",
    "        super().__init__()\n",
    "        self.n_patches = (img_size // patch_size) ** 2\n",
    "        self.proj = nn.Conv2d(in_channels, embed_dim,\n",
    "                              kernel_size=patch_size, stride=patch_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)       # (B, embed_dim, H/P, W/P)\n",
    "        x = x.flatten(2)       # (B, embed_dim, n_patches)\n",
    "        x = x.transpose(1, 2)  # (B, n_patches, embed_dim)\n",
    "        return x\n",
    "\n",
    "\n",
    "class QuantumQKVAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Self-attention where Q, K, V projections run on quantum circuits.\n",
    "    Everything else (attention scores, weighted sum) is classical.\n",
    "    \n",
    "    Flow per token:\n",
    "      embed_dim -[Linear]-> n_qubits -[Quantum Circuit]-> 2^n_qubits -[Linear]-> embed_dim\n",
    "    \"\"\"\n",
    "    def __init__(self, quantum_q, quantum_k, quantum_v,\n",
    "                 embed_dim=64, num_heads=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.quantum_out_dim = 2 ** n_qubits  # 16\n",
    "        \n",
    "        # Classical pre-projection: compress to quantum input size\n",
    "        self.pre_q = nn.Linear(embed_dim, n_qubits)\n",
    "        self.pre_k = nn.Linear(embed_dim, n_qubits)\n",
    "        self.pre_v = nn.Linear(embed_dim, n_qubits)\n",
    "        \n",
    "        # Quantum QKV circuits\n",
    "        self.quantum_q = quantum_q\n",
    "        self.quantum_k = quantum_k\n",
    "        self.quantum_v = quantum_v\n",
    "        \n",
    "        # Classical post-projection: expand quantum output to embed_dim\n",
    "        self.post_q = nn.Linear(self.quantum_out_dim, embed_dim)\n",
    "        self.post_k = nn.Linear(self.quantum_out_dim, embed_dim)\n",
    "        self.post_v = nn.Linear(self.quantum_out_dim, embed_dim)\n",
    "        \n",
    "        # Output projection\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, S, _ = x.shape\n",
    "        \n",
    "        # Classical pre-projection -> scale to [-pi, pi] for angle encoding\n",
    "        q_in = torch.tanh(self.pre_q(x)) * np.pi\n",
    "        k_in = torch.tanh(self.pre_k(x)) * np.pi\n",
    "        v_in = torch.tanh(self.pre_v(x)) * np.pi\n",
    "        \n",
    "        # Flatten batch and sequence dims for quantum processing\n",
    "        q_flat = q_in.reshape(-1, n_qubits)\n",
    "        k_flat = k_in.reshape(-1, n_qubits)\n",
    "        v_flat = v_in.reshape(-1, n_qubits)\n",
    "        \n",
    "        # ---- QUANTUM: QKV projections through parameterized circuits ----\n",
    "        q_out = self.quantum_q(q_flat)  # (B*S, 16)\n",
    "        k_out = self.quantum_k(k_flat)\n",
    "        v_out = self.quantum_v(v_flat)\n",
    "        \n",
    "        # Classical post-projection back to embed_dim\n",
    "        Q = self.post_q(q_out.reshape(B, S, self.quantum_out_dim))\n",
    "        K = self.post_k(k_out.reshape(B, S, self.quantum_out_dim))\n",
    "        V = self.post_v(v_out.reshape(B, S, self.quantum_out_dim))\n",
    "        \n",
    "        # ---- CLASSICAL: Standard multi-head attention computation ----\n",
    "        Q = Q.reshape(B, S, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = K.reshape(B, S, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = V.reshape(B, S, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        attn = (Q @ K.transpose(-2, -1)) * self.scale\n",
    "        attn = torch.softmax(attn, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        \n",
    "        out = (attn @ V).transpose(1, 2).contiguous().reshape(B, S, self.embed_dim)\n",
    "        return self.dropout(self.out_proj(out))\n",
    "\n",
    "\n",
    "class ClassicalAttention(nn.Module):\n",
    "    \"\"\"Standard classical multi-head self-attention (no quantum)\"\"\"\n",
    "    def __init__(self, embed_dim=64, num_heads=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.embed_dim = embed_dim\n",
    "        self.qkv = nn.Linear(embed_dim, 3 * embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, S, D = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, S, 3, self.num_heads, self.head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)  # (3, B, heads, S, head_dim)\n",
    "        Q, K, V = qkv[0], qkv[1], qkv[2]\n",
    "        \n",
    "        attn = (Q @ K.transpose(-2, -1)) * self.scale\n",
    "        attn = torch.softmax(attn, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        \n",
    "        out = (attn @ V).transpose(1, 2).contiguous().reshape(B, S, D)\n",
    "        return self.dropout(self.out_proj(out))\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Transformer encoder block (works with either attention type)\"\"\"\n",
    "    def __init__(self, attention, embed_dim=64, mlp_ratio=2.0, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = attention\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        mlp_dim = int(embed_dim * mlp_ratio)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, mlp_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(mlp_dim, embed_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))   # Attention + residual\n",
    "        x = x + self.mlp(self.norm2(x))    # FFN + residual\n",
    "        return x\n",
    "\n",
    "\n",
    "class HybridViT_QNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Hybrid Quantum-Classical Vision Transformer for image classification.\n",
    "    \n",
    "    Quantum processing is limited to the QKV projections in Block 1.\n",
    "    All other computation (patch embed, attention scores, FFN, classifier) is classical.\n",
    "    \"\"\"\n",
    "    def __init__(self, quantum_q, quantum_k, quantum_v, n_classes=4,\n",
    "                 img_size=32, patch_size=8, embed_dim=64, num_heads=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Patch embedding (Classical)\n",
    "        self.patch_embed = PatchEmbedding(img_size, patch_size, 3, embed_dim)\n",
    "        n_patches = self.patch_embed.n_patches\n",
    "        \n",
    "        # Positional encoding (Classical)\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, n_patches, embed_dim) * 0.02)\n",
    "        self.pos_drop = nn.Dropout(0.1)\n",
    "        \n",
    "        # Block 1: HYBRID - Quantum QKV + Classical attention + Classical FFN\n",
    "        quantum_attn = QuantumQKVAttention(\n",
    "            quantum_q, quantum_k, quantum_v, embed_dim, num_heads\n",
    "        )\n",
    "        self.hybrid_block = TransformerBlock(quantum_attn, embed_dim)\n",
    "        \n",
    "        # Block 2: CLASSICAL - Standard attention + FFN\n",
    "        classical_attn = ClassicalAttention(embed_dim, num_heads)\n",
    "        self.classical_block = TransformerBlock(classical_attn, embed_dim)\n",
    "        \n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        # Classification head (Classical)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 128),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, n_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Patch embedding (Classical)\n",
    "        x = self.patch_embed(x)   # (B, n_patches, embed_dim)\n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        x = self.hybrid_block(x)    # Block 1: Quantum QKV projections\n",
    "        x = self.classical_block(x) # Block 2: Fully classical\n",
    "        \n",
    "        x = self.norm(x)\n",
    "        x = x.mean(dim=1)  # Global average pooling\n",
    "        return self.head(x)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# Instantiate the model\n",
    "# ==============================================================================\n",
    "model = HybridViT_QNN(\n",
    "    quantum_q=quantum_q,\n",
    "    quantum_k=quantum_k,\n",
    "    quantum_v=quantum_v,\n",
    "    n_classes=len(SELECTED_CLASSES),\n",
    "    img_size=32,\n",
    "    patch_size=16,    # 16x16 patches -> 4 patches (4Ã— faster than 8x8!)\n",
    "    embed_dim=64,\n",
    "    num_heads=4\n",
    ").to(device)\n",
    "\n",
    "# Print architecture summary\n",
    "print(\"=\" * 60)\n",
    "print(\"HYBRID QUANTUM-CLASSICAL VISION TRANSFORMER\")\n",
    "print(\"=\" * 60)\n",
    "n_patches = (32 // 16) ** 2\n",
    "print(f\"Patch size:            16x16 -> {n_patches} patches per image\")\n",
    "print(f\"Embedding dim:         64\")\n",
    "print(f\"Attention heads:       4 (head_dim=16)\")\n",
    "print(f\"Block 1 (Hybrid):      Quantum QKV ({n_qubits}q, {2**n_qubits} output) + Classical attention\")\n",
    "print(f\"Block 2 (Classical):   Standard self-attention\")\n",
    "print(f\"Classification:        MLP (64 -> 128 -> {len(SELECTED_CLASSES)})\")\n",
    "print(\"-\" * 60)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "quantum_params = sum(p.numel() for p in quantum_q.parameters()) * 3\n",
    "classical_params = total_params - quantum_params\n",
    "print(f\"Total parameters:      {total_params:,}\")\n",
    "print(f\"  Quantum parameters:  {quantum_params:,} (Q + K + V circuits)\")\n",
    "print(f\"  Classical parameters:{classical_params:,}\")\n",
    "print(f\"Quantum evals/image:   {n_patches} patches x 3 circuits = {n_patches * 3} âš¡\")\n",
    "print(f\"Device:                {device}\")\n",
    "print(f\"ðŸš€ 4Ã— SPEEDUP from larger patches (4 vs 16 patches)\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "900923ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training functions ready!\n",
      "Optimizer:  Adam (lr=0.0005)\n",
      "Scheduler:  ReduceLROnPlateau (factor=0.5, patience=2)\n",
      "Grad clip:  max_norm=1.0\n",
      "Progress:   tqdm with batch-level live plotting\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Training Setup\n",
    "# ==============================================================================\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)  # Lower LR for quantum stability\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=2, \n",
    ")\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer, pbar=None):\n",
    "    \"\"\"Train for one epoch with batch-level metrics tracking\"\"\"\n",
    "    model.train()\n",
    "    batch_losses = []\n",
    "    batch_accs = []\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping: stabilizes quantum parameter shift gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track batch-level metrics\n",
    "        batch_losses.append(loss.item())\n",
    "        _, predicted = outputs.max(1)\n",
    "        batch_acc = 100. * predicted.eq(labels).sum().item() / labels.size(0)\n",
    "        batch_accs.append(batch_acc)\n",
    "        \n",
    "        # Update progress bar\n",
    "        if pbar is not None:\n",
    "            pbar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'acc': f'{batch_acc:.1f}%'\n",
    "            })\n",
    "            pbar.update(1)\n",
    "    \n",
    "    return batch_losses, batch_accs\n",
    "\n",
    "def validate(model, loader, criterion, pbar=None):\n",
    "    \"\"\"Validate with batch-level metrics tracking\"\"\"\n",
    "    model.eval()\n",
    "    batch_losses = []\n",
    "    batch_accs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            batch_losses.append(loss.item())\n",
    "            _, predicted = outputs.max(1)\n",
    "            batch_acc = 100. * predicted.eq(labels).sum().item() / labels.size(0)\n",
    "            batch_accs.append(batch_acc)\n",
    "            \n",
    "            if pbar is not None:\n",
    "                pbar.set_postfix({\n",
    "                    'loss': f'{loss.item():.4f}',\n",
    "                    'acc': f'{batch_acc:.1f}%'\n",
    "                })\n",
    "                pbar.update(1)\n",
    "    \n",
    "    return batch_losses, batch_accs\n",
    "\n",
    "print(\"Training functions ready!\")\n",
    "print(f\"Optimizer:  Adam (lr={optimizer.param_groups[0]['lr']})\")\n",
    "print(f\"Scheduler:  ReduceLROnPlateau (factor=0.5, patience=2)\")\n",
    "print(f\"Grad clip:  max_norm=1.0\")\n",
    "print(f\"Progress:   tqdm with batch-level live plotting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e080776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting training for 10 epochs...\n",
      "Train batches: 433, Val batches: 109\n",
      "Quantum circuit calls per batch: 12 = 12 (4 patches Ã— 3 circuits)\n",
      "Plot updates: Every 5 batches\n",
      "\n",
      "\n",
      "============================================================\n",
      "Epoch 1/10\n",
      "============================================================\n",
      "ðŸ”µ Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Train]:  13%|â–ˆâ–ˆâ–Œ                | 58/433 [13:32<1:26:05, 13.77s/it, loss=1.0305, acc=37.5%]"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Training Loop with REAL-TIME Batch-Level Live Plotting\n",
    "# ==============================================================================\n",
    "\n",
    "n_epochs = 10\n",
    "update_plot_every = 5  # Update plot every N batches for smooth visualization\n",
    "\n",
    "# Store ALL batch-level metrics for high-resolution plotting\n",
    "all_train_losses = []\n",
    "all_train_accs = []\n",
    "all_val_losses = []\n",
    "all_val_accs = []\n",
    "epoch_boundaries = [0]  # Track where epochs start in batch indices\n",
    "epoch_times = []\n",
    "\n",
    "print(f\"ðŸš€ Starting training for {n_epochs} epochs...\")\n",
    "print(f\"Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")\n",
    "print(f\"Quantum circuit calls per batch: {(32//16)**2 * 3} = {(32//16)**2 * 3} (4 patches Ã— 3 circuits)\")\n",
    "print(f\"Plot updates: Every {update_plot_every} batches\")\n",
    "print()\n",
    "\n",
    "%matplotlib inline\n",
    "from IPython import display\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "ax1, ax2, ax3, ax4 = axes.flatten()\n",
    "\n",
    "total_batches_processed = 0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # ============ TRAINING ============\n",
    "    print(\"ðŸ”µ Training...\")\n",
    "    train_pbar = tqdm(total=len(train_loader), desc=f\"Epoch {epoch+1} [Train]\", \n",
    "                      ncols=100, leave=False)\n",
    "    \n",
    "    batch_train_losses, batch_train_accs = train_epoch(\n",
    "        model, train_loader, criterion, optimizer, train_pbar\n",
    "    )\n",
    "    train_pbar.close()\n",
    "    \n",
    "    all_train_losses.extend(batch_train_losses)\n",
    "    all_train_accs.extend(batch_train_accs)\n",
    "    total_batches_processed += len(batch_train_losses)\n",
    "    \n",
    "    # ============ VALIDATION ============\n",
    "    print(\"ðŸ”´ Validating...\")\n",
    "    val_pbar = tqdm(total=len(val_loader), desc=f\"Epoch {epoch+1} [Val]  \", \n",
    "                    ncols=100, leave=False)\n",
    "    \n",
    "    batch_val_losses, batch_val_accs = validate(\n",
    "        model, val_loader, criterion, val_pbar\n",
    "    )\n",
    "    val_pbar.close()\n",
    "    \n",
    "    all_val_losses.extend(batch_val_losses)\n",
    "    all_val_accs.extend(batch_val_accs)\n",
    "    epoch_boundaries.append(total_batches_processed)\n",
    "    \n",
    "    # Step the LR scheduler\n",
    "    val_loss_avg = np.mean(batch_val_losses)\n",
    "    scheduler.step(val_loss_avg)\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start\n",
    "    epoch_times.append(epoch_time)\n",
    "    \n",
    "    # ============ EPOCH SUMMARY ============\n",
    "    train_loss_avg = np.mean(batch_train_losses)\n",
    "    train_acc_avg = np.mean(batch_train_accs)\n",
    "    val_acc_avg = np.mean(batch_val_accs)\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Epoch {epoch+1} Summary:\")\n",
    "    print(f\"  Train Loss: {train_loss_avg:.4f} | Train Acc: {train_acc_avg:.2f}%\")\n",
    "    print(f\"  Val Loss:   {val_loss_avg:.4f} | Val Acc:   {val_acc_avg:.2f}%\")\n",
    "    print(f\"  Time:       {epoch_time:.1f}s | LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    \n",
    "    # ============ UPDATE PLOTS ============\n",
    "    ax1.clear()\n",
    "    ax2.clear()\n",
    "    ax3.clear()\n",
    "    ax4.clear()\n",
    "    \n",
    "    train_batch_indices = list(range(len(all_train_losses)))\n",
    "    val_batch_indices = list(range(len(all_val_losses)))\n",
    "    \n",
    "    # Plot 1: Batch-level training loss (high resolution)\n",
    "    ax1.plot(train_batch_indices, all_train_losses, 'b-', alpha=0.3, linewidth=0.5)\n",
    "    # Moving average for trend\n",
    "    if len(all_train_losses) > 10:\n",
    "        window = min(20, len(all_train_losses) // 5)\n",
    "        train_loss_smooth = np.convolve(all_train_losses, \n",
    "                                         np.ones(window)/window, mode='valid')\n",
    "        ax1.plot(range(window-1, len(all_train_losses)), train_loss_smooth, \n",
    "                 'b-', linewidth=2, label='Train Loss (smoothed)')\n",
    "    ax1.set_xlabel('Training Batch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('Batch-Level Training Loss', fontweight='bold')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Plot 2: Batch-level validation loss\n",
    "    ax2.plot(val_batch_indices, all_val_losses, 'r-', alpha=0.5, linewidth=1)\n",
    "    if len(all_val_losses) > 10:\n",
    "        window = min(20, len(all_val_losses) // 5)\n",
    "        val_loss_smooth = np.convolve(all_val_losses, \n",
    "                                       np.ones(window)/window, mode='valid')\n",
    "        ax2.plot(range(window-1, len(all_val_losses)), val_loss_smooth, \n",
    "                 'r-', linewidth=2, label='Val Loss (smoothed)')\n",
    "    ax2.set_xlabel('Validation Batch')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.set_title('Batch-Level Validation Loss', fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.legend()\n",
    "    \n",
    "    # Plot 3: Batch-level training accuracy\n",
    "    ax3.plot(train_batch_indices, all_train_accs, 'b-', alpha=0.3, linewidth=0.5)\n",
    "    if len(all_train_accs) > 10:\n",
    "        window = min(20, len(all_train_accs) // 5)\n",
    "        train_acc_smooth = np.convolve(all_train_accs, \n",
    "                                        np.ones(window)/window, mode='valid')\n",
    "        ax3.plot(range(window-1, len(all_train_accs)), train_acc_smooth, \n",
    "                 'b-', linewidth=2, label='Train Acc (smoothed)')\n",
    "    ax3.set_xlabel('Training Batch')\n",
    "    ax3.set_ylabel('Accuracy (%)')\n",
    "    ax3.set_title('Batch-Level Training Accuracy', fontweight='bold')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    ax3.legend()\n",
    "    \n",
    "    # Plot 4: Batch-level validation accuracy\n",
    "    ax4.plot(val_batch_indices, all_val_accs, 'r-', alpha=0.5, linewidth=1)\n",
    "    if len(all_val_accs) > 10:\n",
    "        window = min(20, len(all_val_accs) // 5)\n",
    "        val_acc_smooth = np.convolve(all_val_accs, \n",
    "                                      np.ones(window)/window, mode='valid')\n",
    "        ax4.plot(range(window-1, len(all_val_accs)), val_acc_smooth, \n",
    "                 'r-', linewidth=2, label='Val Acc (smoothed)')\n",
    "    ax4.set_xlabel('Validation Batch')\n",
    "    ax4.set_ylabel('Accuracy (%)')\n",
    "    ax4.set_title('Batch-Level Validation Accuracy', fontweight='bold')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    ax4.legend()\n",
    "    \n",
    "    # Draw epoch boundaries on training plots\n",
    "    for boundary in epoch_boundaries[1:-1]:  # Skip first (0) and last (current)\n",
    "        ax1.axvline(x=boundary, color='gray', linestyle='--', alpha=0.3, linewidth=1)\n",
    "        ax3.axvline(x=boundary, color='gray', linestyle='--', alpha=0.3, linewidth=1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(fig)\n",
    "\n",
    "# ============ FINAL SUMMARY ============\n",
    "total_time = sum(epoch_times)\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"âœ… TRAINING COMPLETE!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total time:        {total_time:.1f}s ({total_time/60:.1f} min)\")\n",
    "print(f\"Avg time/epoch:    {total_time/n_epochs:.1f}s\")\n",
    "\n",
    "# Calculate epoch-level metrics for final summary\n",
    "epoch_val_accs = []\n",
    "for i in range(len(epoch_boundaries) - 1):\n",
    "    start_batch = epoch_boundaries[i] - (i * len(val_loader))\n",
    "    end_batch = epoch_boundaries[i+1] - ((i+1) * len(val_loader))\n",
    "    if i == 0:\n",
    "        start_batch = 0\n",
    "        end_batch = len(val_loader)\n",
    "    else:\n",
    "        start_batch = i * len(val_loader)\n",
    "        end_batch = (i + 1) * len(val_loader)\n",
    "    if end_batch <= len(all_val_accs):\n",
    "        epoch_val_accs.append(np.mean(all_val_accs[start_batch:end_batch]))\n",
    "\n",
    "if epoch_val_accs:\n",
    "    best_val_acc = max(epoch_val_accs)\n",
    "    best_epoch = epoch_val_accs.index(best_val_acc) + 1\n",
    "    print(f\"Best Val Accuracy: {best_val_acc:.2f}% (epoch {best_epoch})\")\n",
    "    print(f\"Final Val Acc:     {epoch_val_accs[-1]:.2f}%\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Total batches:   {len(all_train_losses)} training, {len(all_val_losses)} validation\")\n",
    "print(f\"ðŸš€ Speedup factor:  4Ã— (from 16Ã—3=48 â†’ 4Ã—3=12 quantum calls/image)\")\n",
    "print(f\"{'='*60}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".qcompute (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
